{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPI6CC88IAQ4qF1+qkH0YaH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grfaith/AmericanStories/blob/main/first_chunks_preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgI0DiyxZ9kJ",
        "outputId": "644e8a93-6c66-4d6f-8b25-423d50d3bb3f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "Dq_AhHl7Zz8z",
        "outputId": "84d1c45e-b3c4-4152-b1fc-fbd9c8537391"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Initialize an empty list to hold the dataframes\\ndfs = []\\n\\n# Iterate over the files and read them into dataframes\\nfor i, file in enumerate(files):\\n    file_path = os.path.join(output_dir, file)\\n    \\n    # Read the first file with headers\\n    if i == 0:\\n        df = pd.read_csv(file_path, low_memory=False)  # Use the column names from the first file\\n    else:\\n        # Read subsequent files without headers\\n        df = pd.read_csv(file_path, header=None, low_memory=False)\\n        df.columns = dfs[0].columns  # Ensure the columns match\\n\\n    dfs.append(df)\\n\\n# Concatenate all the dataframes\\nconcatenated_df = pd.concat(dfs, ignore_index=True)\\n\\n# Display the result (or you can save it to a file)\\nprint(concatenated_df)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Specify the path to the output directory\n",
        "output_dir = '/content/drive/My Drive/output'\n",
        "\n",
        "# Get a list of all files in the directory matching the format output_chunk_N\n",
        "files = [f for f in os.listdir(output_dir) if f.startswith('output_chunk_') and f.endswith('.csv')]\n",
        "\n",
        "# Sort files based on the number N in output_chunk_N\n",
        "files.sort(key=lambda f: int(f.split('_')[-1].split('.')[0]))\n",
        "\n",
        "'''\n",
        "# Initialize an empty list to hold the dataframes\n",
        "dfs = []\n",
        "\n",
        "# Iterate over the files and read them into dataframes\n",
        "for i, file in enumerate(files):\n",
        "    file_path = os.path.join(output_dir, file)\n",
        "\n",
        "    # Read the first file with headers\n",
        "    if i == 0:\n",
        "        df = pd.read_csv(file_path, low_memory=False)  # Use the column names from the first file\n",
        "    else:\n",
        "        # Read subsequent files without headers\n",
        "        df = pd.read_csv(file_path, header=None, low_memory=False)\n",
        "        df.columns = dfs[0].columns  # Ensure the columns match\n",
        "\n",
        "    dfs.append(df)\n",
        "\n",
        "# Concatenate all the dataframes\n",
        "concatenated_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Display the result (or you can save it to a file)\n",
        "print(concatenated_df)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###  ***Transplanted code from AS_Explore_Postprocess_For_Commit.ipynb ***\n",
        "###  This code runs the light processing routines from American Stories on AS-format article text.\n",
        "###  The file is loaded and saved locally."
      ],
      "metadata": {
        "id": "3WdfwrHBa5d5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installs\n",
        "\n",
        "!pip install symspellpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nt48Ln4zbTV1",
        "outputId": "44d4fce5-fadb-4fdb-c77e-af89ba631934"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting symspellpy\n",
            "  Downloading symspellpy-6.7.8-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting editdistpy>=0.1.3 (from symspellpy)\n",
            "  Downloading editdistpy-0.1.5-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
            "Downloading symspellpy-6.7.8-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading editdistpy-0.1.5-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.1/144.1 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: editdistpy, symspellpy\n",
            "Successfully installed editdistpy-0.1.5 symspellpy-6.7.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports\n",
        "import io\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import tqdm as tq"
      ],
      "metadata": {
        "id": "gamjzqXZbTQL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's initialize the package\n",
        "import pkg_resources\n",
        "from symspellpy import SymSpell, Verbosity\n",
        "import string\n",
        "\n",
        "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "en_dict = pkg_resources.resource_filename('symspellpy', 'frequency_dictionary_en_82_765.txt')\n",
        "sym_spell.load_dictionary(en_dict, term_index=0, count_index=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sna0otyQbTKq",
        "outputId": "76b55dcb-78c1-43a5-fd33-016d2de393f7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is code from AS and has functions for text correction and polishing.\n",
        "# we now create a few functions that take care of the issues flagged above\n",
        "\n",
        "# thsese two functions implement spelling corrections\n",
        "def check_word(word):\n",
        "  no_punc_word = word.strip(string.punctuation)\n",
        "  if len(no_punc_word) > 0:\n",
        "    suggestions = sym_spell.lookup(no_punc_word, Verbosity.CLOSEST, max_edit_distance=1, include_unknown=True, transfer_casing=True)\n",
        "  else:\n",
        "    return word\n",
        "  return word.replace(no_punc_word, suggestions[0].term)\n",
        "\n",
        "def spell_check(text):\n",
        "  lines = text.split('\\n')\n",
        "  checked_lines = []\n",
        "  for line in lines:\n",
        "    words = line.split(' ')\n",
        "    checked_line = ' '.join([check_word(word) for word in words])\n",
        "    checked_lines.append(checked_line)\n",
        "  return '\\n'.join(checked_lines)\n",
        "\n",
        "# this function checks capitalization\n",
        "def capitalization_check(text):\n",
        "  lines = text.split('\\n')\n",
        "  checked_lines = []\n",
        "  for line in lines:\n",
        "    words = line.split(' ')\n",
        "    for i in range(1, len(words)):\n",
        "      if words[i-1][-1] in ['.', '!', '?']:\n",
        "        words[i] = words[i].capitalize()\n",
        "      else:\n",
        "        no_punc_word = words[i].strip(string.punctuation)\n",
        "        if no_punc_word in sym_spell.words and no_punc_word not in ['i', \"i'll\"]: # Check that the word is not a propper noun\n",
        "          words[i] = words[i].replace(no_punc_word, no_punc_word.lower())\n",
        "\n",
        "    checked_lines.append(' '.join(words))\n",
        "  return '\\n'.join(checked_lines)\n",
        "\n",
        "# this functions corrects line breaks\n",
        "def line_merge(text):\n",
        "  lines = [l.split() for l in text.split('\\n')]\n",
        "  for i in range(len(lines) - 1):\n",
        "    if len(lines[i]) == 0 or len(lines[i+1]) == 0:\n",
        "      continue\n",
        "    elif lines[i][-1][-1] == '-': # Automatically merge if a line ends with a hyphen\n",
        "      lines[i][-1] = lines[i][-1][:-1] + lines[i+1][0]\n",
        "      lines[i+1] = lines[i+1][1:]\n",
        "    elif lines[i][-1].strip(string.punctuation).lower() not in sym_spell.words or lines[i+1][0].strip(string.punctuation).lower() not in sym_spell.words:\n",
        "      if (lines[i][-1].strip(string.punctuation).lower() + lines[i+1][0].strip(string.punctuation).lower()) in sym_spell.words:\n",
        "        lines[i][-1] += lines[i+1][0]\n",
        "        lines[i+1] = lines[i+1][1:]\n",
        "\n",
        "  return '\\n'.join([' '.join(l) for l in lines])\n"
      ],
      "metadata": {
        "id": "tKuFZITpbTF-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this functions implements all three methods\n",
        "def postprocess(text):\n",
        "  merged = line_merge(text)\n",
        "  checked = spell_check(merged)\n",
        "  capitalization_normalized = capitalization_check(checked)\n",
        "  return capitalization_normalized"
      ],
      "metadata": {
        "id": "Pa2a03C8bTBY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Specify the path to the output directory\n",
        "output_dir = '/content/drive/My Drive/output'\n",
        "\n",
        "# Get a list of all files in the directory matching the format output_chunk_N\n",
        "files = [f for f in os.listdir(output_dir) if f.startswith('output_chunk_') and f.endswith('.csv')]\n",
        "\n",
        "# Sort files based on the number N in output_chunk_N and remove any duplicates using set\n",
        "files = sorted(list(set(files)), key=lambda f: int(f.split('_')[-1].split('.')[0]))\n",
        "\n",
        "# Ensure 'postprocess' function is defined\n",
        "def postprocess(article):\n",
        "    # Example postprocess logic (replace this with your actual logic)\n",
        "    return article.lower()  # Dummy example: converting to lowercase\n",
        "\n",
        "# Define a function to process each file\n",
        "def process_file(file):\n",
        "    file_path = os.path.join(output_dir, file)\n",
        "\n",
        "    # Read the file\n",
        "    df = pd.read_csv(file_path, low_memory=False)\n",
        "\n",
        "    # Apply the postprocess function to the 'article_text' column\n",
        "    df['article_p'] = df['article_text'].apply(lambda x: postprocess(x) if not pd.isnull(x) else x)\n",
        "\n",
        "    # Save the processed file (you can overwrite the file or create a new one)\n",
        "    output_file_path = os.path.join(output_dir, f'processed_{file}')  # Change to your preferred output\n",
        "    df.to_csv(output_file_path, index=False)\n",
        "\n",
        "    print(f'{file} has been processed and saved to {output_file_path}')\n",
        "\n",
        "# Use ThreadPoolExecutor to process files in parallel\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    # Submit each file for processing\n",
        "    executor.map(process_file, files)\n",
        "\n",
        "print(\"All files have been processed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZhlH_9KbStW",
        "outputId": "3cc4b831-0a32-4b2d-f8c3-496befc22257"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output_chunk_6.csv has been processed and saved to /content/drive/My Drive/output/processed_output_chunk_6.csv\n",
            "output_chunk_5.csv has been processed and saved to /content/drive/My Drive/output/processed_output_chunk_5.csv\n",
            "output_chunk_4.csv has been processed and saved to /content/drive/My Drive/output/processed_output_chunk_4.csv\n",
            "output_chunk_3.csv has been processed and saved to /content/drive/My Drive/output/processed_output_chunk_3.csv\n",
            "output_chunk_2.csv has been processed and saved to /content/drive/My Drive/output/processed_output_chunk_2.csv\n",
            "output_chunk_11.csv has been processed and saved to /content/drive/My Drive/output/processed_output_chunk_11.csv\n",
            "output_chunk_1.csv has been processed and saved to /content/drive/My Drive/output/processed_output_chunk_1.csv\n",
            "output_chunk_7.csv has been processed and saved to /content/drive/My Drive/output/processed_output_chunk_7.csv\n",
            "output_chunk_8.csv has been processed and saved to /content/drive/My Drive/output/processed_output_chunk_8.csv\n",
            "output_chunk_9.csv has been processed and saved to /content/drive/My Drive/output/processed_output_chunk_9.csv\n",
            "output_chunk_10.csv has been processed and saved to /content/drive/My Drive/output/processed_output_chunk_10.csv\n",
            "All files have been processed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check_df = pd.read_csv('/content/drive/My Drive/output/processed_output_chunk_1.csv')\n",
        "print (check_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D4XUeGnhnzT",
        "outputId": "190a2a84-e869-47e3-d2fe-9a136db61763"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          article_ID  row_number  aether  \\\n",
            "0  6_1799-01-28_p4_sn83045634_00332895059_1799012...         107       0   \n",
            "1  8_1799-04-15_p4_sn83045634_00332895059_1799041...         316       0   \n",
            "2  5_1802-05-31_p1_sn83016063_00332895060_1802053...         152       0   \n",
            "3  13_1802-07-12_p4_sn83016063_00332895060_180207...         148       0   \n",
            "4  21_1803-05-30_p4_sn83016072_00332895060_180305...         384       0   \n",
            "\n",
            "   asteroid  aurora  borealis  celestial  comet  copernicus  cosmic  ...  \\\n",
            "0         0       0         0          0      0           0       0  ...   \n",
            "1         0       0         0          0      0           0       0  ...   \n",
            "2         0       3         0          0      0           0       0  ...   \n",
            "3         0       0         0          0      0           0       0  ...   \n",
            "4         0       0         0          0      0           0       0  ...   \n",
            "\n",
            "   uranus  venus  venus transit  total_keyword_count    pub_date  pub_year  \\\n",
            "0       0      0              0                    4  1799-01-28      1799   \n",
            "1       0      0              0                    2  1799-04-15      1799   \n",
            "2       0      0              0                    3  1802-05-31      1802   \n",
            "3       0      0              0                    2  1802-07-12      1802   \n",
            "4       0      0              0                    2  1803-05-30      1803   \n",
            "\n",
            "                                        article_text  \\\n",
            "0  \\n\\n\\nTHE fcv,n Gags OF lft, from the frG dawn...   \n",
            "1  Thirteen AllegOrleS.\\n\\n\\n7. HUMAN thoughts ar...   \n",
            "2  lhe Min'HerahUs now aPoJogIze for the\\nglariag...   \n",
            "3  1eNTAINING all the Tables neceAary to be uied\\...   \n",
            "4  Of lhe bea qualify which will be % on\\nrez!ona...   \n",
            "\n",
            "                       bbox  \\\n",
            "0  [1365, 3697, 2554, 8227]   \n",
            "1  [1432, 1649, 2632, 7845]   \n",
            "2  [3973, 1882, 5268, 9174]   \n",
            "3   [286, 6477, 1581, 8294]   \n",
            "4   [1287, 720, 2358, 5755]   \n",
            "\n",
            "                                        article_link  \\\n",
            "0  https://www.loc.gov/resource/sn83045634/1799-0...   \n",
            "1  https://www.loc.gov/resource/sn83045634/1799-0...   \n",
            "2  https://www.loc.gov/resource/sn83016063/1802-0...   \n",
            "3  https://www.loc.gov/resource/sn83016063/1802-0...   \n",
            "4  https://www.loc.gov/resource/sn83016072/1803-0...   \n",
            "\n",
            "                                           article_p  \n",
            "0  \\n\\n\\nthe fcv,n gags of lft, from the frg dawn...  \n",
            "1  thirteen allegorles.\\n\\n\\n7. human thoughts ar...  \n",
            "2  lhe min'herahus now apojogize for the\\nglariag...  \n",
            "3  1entaining all the tables neceaary to be uied\\...  \n",
            "4  of lhe bea qualify which will be % on\\nrez!ona...  \n",
            "\n",
            "[5 rows x 71 columns]\n"
          ]
        }
      ]
    }
  ]
}